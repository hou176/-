# pandas数据处理

### 建模流程  
- 业务抽象为分类&回归问题  （两种问题）
	- 定义标签（找到目标值）
	- 选取样本（选取特征）
	- 特征工程 + 模型训练 + 模型评价调优
	- 输出报告
	- 上线开始监控
- 特征
- 数据预处理
	- 数据清洗
	- 数据抽样
	- 数据转换

### 数据清洗
####缺失值处理
- 数据缺分类
	- 行记录丢失  就是没有数据了 空了一行
	- 数据库是null
	- python返回的对象null
	- pandas numpy nan  也是一种空的情况
	- 个别情况会用字符串代替缺失值  而且  有缺失值也不一定是坏事 没准就有用
- 缺失值处理
	- 直接删除  删除正一行或者字段 但是有一些不适合删除
		- 数据不完整的太多了  太大了 超过%10 删除损失有用信息
		- 缺失值的数据记录大量存在明显的数据分布规律或者特征  同时还有有用的信息
	- 填充缺失值
		- 对于数据使用均值  加权均值  中位数  众数最多之进行填充
		- 面模型法对缺失字段作为目标变量进行预测  可能会补全如果是数值变量 采用回归补全 分类就是用分类模型补全
		- 专家补全  就是用专家
		- 随机森林 特殊值法  多重填补
	- 真值转化 承认缺失值存在 他就是正常的一部分  比如性别  就是男 女  未知
	- 不处理 有些模型对于缺失值就与容忍度的模型：KNN  决策树  随机森林 神经网络   朴素贝叶斯   KNN模型缺失值不参与距离计算
- 缺失值的处理套路  找到缺失值后分析样本查看占比 选择合适的方式处理缺失值
-异常值处理（极大值极小值）
	- 处于特定的分布区域或者范围之外的数据通常被定义噪音
	- 大多数情况下噪音是要被踢出的以避免其对总体数据评估和分析挖掘影响 但是有几种情况时不要进行处理的
		- 比如正常是1000数值  但是昨天促销  就变成10000  今天仓库不够了  就变成100  这样 是正常情况
		- 异常监测模型 围绕异常值展开的分析工作 比如羊毛党的识别  作弊的流量 信用卡诈骗   
		- 还有对异常值不敏感的数据模型   决策树就是这样
- 重复值的处理
	- 样本不均衡时候故意重复采样的数据   过采样就是这样
	- 事务性数据  尤其与钱的相关的数据   比如重复的订单  重复出库的申请

#### 对数据进行清洗

- 缺失值
	- dataframe.isnull() 是否缺失值
	- dataframe.dropna() 删除缺失值
	- dataframe.fillna() 填充缺失值
- 异常值处理
dataframe.mean()  计算平均值
dataframe.std()   计算标准差
- 判断异常值
	- 计算公式 Z = X-μ/σ  其中μ为总体平均值，X-μ为离均差，σ表示标准差。z的绝对值表示在标准差范围内的原始分数与总体均值之间的距离。当原始分数低于平均值时，z为负，以上为正。
	- 如何判断是异常  其实就是那个2.2   有固定的的业务规则的直接用  没有的话使用数学模型进行判断  如正态分布的标准范围 分位数法等
- 数据去重
dataframe.duplicated()  判断重复数据记录
dataframe.drop_duplicates()  删除数据记录所有列值相同的记录

####数值型数据的处理
- 数据标准化是一个常用的数据预处理操作  处理不同规模的量纲数据 使其缩放到相同的数据区间和范围  减少规模特征分布差异等对模型的影响
- 标准化就是用的均值和标准差  值=（处理值-所在列的均值）/标准差 适合大多数类型的数据  也是很多工具默认的标准化方法  但是S-Zcore是中心化的方法 会改变原来的分布结构  不适合对稀疏数据做处理
- 归一化
	- max--min标准化方法是对原始数据进行线性变换x'=(x-min)/(max-min)     得到的值会完全落入[0,1]区间  同时还能较好的保持原有数据结构
- 如果是树模型就不用了  线性回归和逻辑回归就要用了标准化和归一化   树模型不用是因为人家要比较大小的（基本）就像五十个人和马云比  他工资无线趋近1  剩下的人无线趋近0

####离散化/分箱/分桶
- 离散化  无线空间有限的个体映射有限的空间中  大多是针对连续数据进行的  处理后数据值域分布将从连续属性变成李三属性  这种属性一般包含两个或者两个以上的值域
	- 节约计算资源  提高计算效率
	- 算法模型计算需要  虽然很多模型  例如决策树可以支持输入连续数据  但是决策树自己会提前的将连续变成离散化的
	- 增强模型稳定性和准确度异常的数据不会明显吐出异常特征  会被划分一个子集的一部分
	- 大多数图形特征检测 都是先将图形做二值化处理  二值化也是离散的一种
	- 模型应用和部署的需要  如果原始数据的值域太多了  不符合逻辑  业务也很难用这个模型
	- 虽然说是对连续的数据进行处理  但是有时候也会处理已经离散数据 因为这些离散化的数据过于复杂   繁琐 甚至不符合业务逻辑
- 常见的离散化场景
	- 时间数据的离散化
	- 针对多值离散数据的离散化  不是要处理数值型数据  就是内容  而是内容的分类或者顺序数据   比如：以前是10个区间  现在要成4个了  就要不他们先合并
	- 也有可能是划分逻辑的问题  需要重新分化  平常就是由于业务逻辑的变更 导致在原始数据中存在不同历史数据下的不同值域定义  比如：以前是 高级 中级  低级  现在是   特高级  超高级  高级  中继  低级  这样就需要从新画了
	- 连续数据是离散化的关键  在分类或者关联分析应用尤其广泛  算法的结果以类别或者属性标示为基础  
		- 第一类结果   区间的集合   
		- 第二类结果   将数据划分为特定类  类1 类2  类3
		- 第一类方法   分位数法  四分位  五分位  十分位 离散化处理
		- 第二种方法   距离区间法可使用等距区间或自定义区间的方式进行离散化
		- 第三种方法   频率区间法   数据按照不同数据评率惊醒排序
		- 第四种方法   聚类法  
	- 二值比  很多场景下 需要用二值比：每个数据点跟阈值比较  大于阈值设置为某一固定值（比如1） 小于阈值（0）  就跟二进制一样
- 数值型数据进行分组   这就是离散化  也算是对连续数据进行分组

####量
- 很多算法无法直接处理非数值的变量
- kmeans算法基于距离的相似度计算  而字符串则无法直接计算出距离  
- 就算法本身支持 很多算法实现也无法基于字符串做矩阵变化
- numpy 以及基于numpy的sklearn虽然允许直接使用和储存字符串型变量  但却无法发挥矩阵的计算的优势
- 一般都是转换成数值类型
-有几种情况然后就出现几  然后在哪一种情况下给个1  真值  sklearn对数值型数据也是一样  但是pandas不会  数值型数据直接就会让他接着成数值型数据
- 时间类型的处理  通过pandas的to_datetime转化成datetime类型  就是时间类型

####样本分布不均匀
- 样本类别分布不均匀
	- 样本类别不分不均匀主要出现在与分类相关的建模问题上不均衡是指不同类别的样本差异非常大
	- 样本分布不均衡将导致样本量少的分了所含的特征过少  建立的模型容易过拟合
	- 如果不同分类间的样本量差异过10倍就要引起警觉了   超过20倍就是出大事了一定想办法解决了
- 分布不均匀的场景
	- 异常检测  大多数的异常个案都是少量的  比如刷单  黄牛订单  信用卡欺骗   这些数据样本所占比例通常是整体样本很少的一部分  以信用卡欺骗为例  刷实体信用卡的比例一般在 0.1%左右
	- 客户的流失  大型企业的流失客户都是比较少的
	- 偶发事件罕见的时间就跟异常检测  都属于少发生的情况  不同就是异常检测通常有预定号的规则和逻辑   会预判发生  但是偶发就不对了   无法判断  也没有明显的积极或者消极的影响
	- 低频事件  这种事件是预期或者计划性事件  频率及其的低  每年双十一购物街都会产生搞得销售额  但是放眼一年来看  占比可能不到 1%  这就是典型的低频事件
- 样本不均衡的问题
	- 过抽样 上采样  增加分类中少数类样本的数量来实现样本均衡 就是复制好多分少的那一部分数据  但是有确定  样本特征少的话可能会导致过拟合问题  经过改进的过抽样方法会在少数类中加入随机噪声、干扰数据，或通过一定规则产生新的合成样本
	- 欠抽样 下采样  其通过减少分类中多数类样本的数量来实现样本均衡，最直接的方法是随机去掉一些多数类样本来减小多数类的规模。缺点是会丢失多数类样本中的一些重要信息。
- 总体上，过抽样和欠抽样更适合大数据分布不均衡的情况，尤其是过抽样方法，应用极为广泛。
- 正负样本的惩罚权重：在算法实现过程中，对于分类中不同样本数量的类别分别赋予不同的权重（一般思路分类中的小样本量类别权重高，大样本量类别权重低），然后进行计算和建模。
  - 很多模型和算法中都有基于类别参数的调整设置，以scikit-learn中的SVM为例，通过在class_weight：{dict，'balanced'}中针对不同类别来手动指定权重，如果使用其默认的方法balanced，那么SVM会将权重设置为与不同类别样本数量呈反比的权重来进行自动均衡处理，计算公式如下：n_samples/(n_classes*np.bincount(y))
- 组合/集成：每次生成训练集时使用所有分类中的小样本量，同时从分类中的大样本量中随机抽取数据来与小样本量合并构成训练集，这样反复多次会得到很多训练集和训练模型。最后在应用时，使用组合方法（例如投票、加权投票等）产生分类预测结果。
  - 数据集中的正、负例的样本分别为100条和10000条，比例为1：100。此时可以将负例样本（类别中的大量样本集）随机分为100份（当然也可以分更多），每份100条数据；然后每次形成训练集时使用所有的正样本（100条）和随机抽取的负样本（100条）形成新的数据集。如此反复可以得到100个训练集和对应的训练模型。
    这种解决问题的思路类似于随机森林。在随机森林中，虽然每个小决策树的分类能力很弱，但是通过大量的“小树”组合形成的“森林”具有良好的模型预测能力

####数据抽样
- 抽样从整体样本中通过一定的方法选择一部分样本  抽样是数据处理的基本步骤之一 也是科学实验  质量检验  社会调查普遍采用的一种经济有效的工作和研究方法
- 抽样的原因
	- 数据量太大了  全量计算对硬件要求有点高
	- 数据采集限制  很多时候抽样从数据采集就已经快开始了  例如社会检查
	- 时效性的要求  抽样带来的是一局部反映全局的思路  如果方法准确  可以极小的计算量来实现对整体数据的统计分析  时效性上大大增加
- 抽样意义
	- 通过抽样来实现快速的概念的验证
	- 通过抽样可以解决样本不均衡的问题（钱抽烟，过抽样以及组合集成的方法）
	- 通过抽样可以解决无法实现对全部样本覆盖的数据分析场景（市场研究  客户线下调研  产品品质检验）
- 如何抽样
	- 非概率
		- 没有原则  没有等概率 而是根据人类的主观经验和状态进行的
	- 概率
		- 简单的随机抽样安等概率原则直接抽取  多少多少个   简单  易于操作  但是不能确保样本能完美的用  不能很好的替代总体  基本前提是所有样本都是等概率分布的  但是真实情况确实多数样本都不是 或者无法判断是否是等概率分布的   会存在重复的数据 该方法适用于个体分布均匀的场景
	- 等距抽样  是多有的样本排序  按照隔多少 在取多少 也是  适用于个体分布均匀或者明显的均匀分布规律  无明显趣事或者周期性规律的数据  总体样本的分布呈现明显的分布规律时容易产生偏差  例如增减趋势  周期性规律
- 抽取数据几个问题 
	- 数据要能反映运营背静
		- 数据时效性  使用过时的数据来分析现在的运营状态
		- 不能缺少关键因素数据
		- 具有业务随机性
		- 业务的增长性
		- 数据来源的多样性
		- 业务数据可行性问题  
- 抽样样本能满足数据分析和建模需求
	- 抽样样本量的问题
		- 时间为维度分布  至少包含一个能满足预测的完整业务周期
		- 做预测分析建模的考虑特征数量和特征值域
		- 关联分析建模的  根据关联前后的数量
		- 对于异常检测分析建模的  无论是监督式还是非监督式建模  由于异常数据本来就是小概率分布  因此异常数据记录一般越多越好
	- 抽样样本不同类别中的分布问题
		- 抽取样本的准确能代表整体体征
		- 非数值的特征值域分布需要与中体一致
		- 数值类型特征的数据分布区间和各个统计量需要与整体数据分布一致
- 异常检测类的数据处理
	- 对于异常检测类的应用要包含全部异常样本。对于异常检测类的分析建模，本来异常数据就非常稀少，因此抽样时要优先将异常数据包含进去。
	- 对于需要去除非业务因素的数据异常，如果有类别特征需要与类别特征分布一致；如果没有类别特征，属于非监督式的学习，则需要与整体分布一致。